<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Dividing Data and Testing Tree Models</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/textmate.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>

<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
  padding-left: 10px;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">tigerTree</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="instructorNotes.html">For Instructors</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="growTree.html">Growing Trees</a>
    </li>
    <li>
      <a href="divideTrainTest.html">Testing Trees</a>
    </li>
    <li>
      <a href="treeDetective.html">The Tree Detective</a>
    </li>
    <li>
      <a href="distAtNodes.html">Distribution at the Nodes</a>
    </li>
    <li>
      <a href="tuneTree.html">Tuning Your Tree:  An App</a>
    </li>
    <li>
      <a href="pruneTree.html">Tuning Your Tree:  Automated</a>
    </li>
  </ul>
</li>
<li>
  <a href="resources.html">Related</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a>
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Dividing Data and Testing Tree Models</h1>

</div>


<div id="the-idea-of-training-and-test" class="section level2">
<h2>The Idea of Training and Test</h2>
<p>When you have made a predictive model it’s important to estimate how well it would perform on <em>new</em> observations—that is, on observations that are different from the ones that were used to build the model in the first place. This cannot be done if you use all of the available data to build your model. Hence it is usually recommended to divide your data into two sets:</p>
<ul>
<li>a training set, and</li>
<li>a test set.</li>
</ul>
<p>The training set will be used to build the model. The model is then tested on the test set. How well it performs on the test set is likely to be a more reliable indicator of its performance “in the field” on new observations than how it performs on the very training set that it was built upon.</p>
<p>In order to remove any possibility of researcher-bias, this division should be performed randomly by the computer.</p>
</div>
<div id="training-and-test-example" class="section level2">
<h2>Training and Test: Example</h2>
<p>We’ll use the <code>verlander</code> data frame from the <code>tigerstats</code> package:</p>
<pre class="r"><code>library(tigerstats)
help(verlander)
str(verlander)</code></pre>
<pre><code>## &#39;data.frame&#39;:    15307 obs. of  12 variables:
##  $ season     : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...
##  $ gamedate   : Date, format: &quot;2009-04-06&quot; &quot;2009-04-06&quot; ...
##  $ pitch_type : Factor w/ 5 levels &quot;CH&quot;,&quot;CU&quot;,&quot;FF&quot;,..: 3 2 2 2 3 2 3 3 3 1 ...
##  $ balls      : int  0 0 1 1 1 1 2 3 0 0 ...
##  $ strikes    : int  0 1 1 2 2 2 2 2 0 1 ...
##  $ pitches    : int  0 1 2 3 4 5 6 7 8 9 ...
##  $ speed      : num  96.6 81.1 80.4 83.1 97.9 82.6 98.7 97.1 97.8 85.8 ...
##  $ px         : num  -0.43 -0.43 -0.17 -0.76 -0.31 0.32 0.72 0.51 -0.45 -0.89 ...
##  $ pz         : num  3.24 3.79 2.98 3.45 2.5 1.3 3.3 2.37 2.29 2.1 ...
##  $ pfx_x      : num  -4.44 5.53 4.83 4.21 -6.64 5.21 -7.65 -5.1 -8.07 -6.67 ...
##  $ pfx_z      : num  9.28 -8.28 -8.03 -9.33 6.3 -8.44 4.77 6.29 7.38 4.47 ...
##  $ batter_hand: Factor w/ 2 levels &quot;L&quot;,&quot;R&quot;: 2 2 2 2 2 2 2 2 2 2 ...</code></pre>
<p>Our goal is to build a model to predict how the pitchFX machine would classify a Justin Verlander pitch. In other words, we would like to predict <strong>pitch_type</strong> from the other factor and numerical variables in the data frame.</p>
<p>Trees can’t work with dates, and the the season won’t be relevant if we want to use the tree to predict pitches for future seasons, so we create a frame without those variables:</p>
<pre class="r"><code>ver2 &lt;- verlander
ver2$season &lt;- NULL
ver2$gamedate &lt;- NULL</code></pre>
<p>Let’s say that we plan to build just one tree model. Then it’s appropriate to divide the data into a training and a test set. We can do with is the <code>divideTrainTest()</code> function:</p>
<pre class="r"><code>myTrainTest &lt;- divideTrainTest(seed = 4040, prop.train = 0.7, data = ver2)</code></pre>
<p>The <code>seed</code> argument can be any integer; setting the seed guarantees the reproducibility of your work while making the division “look random.” Setting <code>prop.train</code> to 0.7 ensures an approximately 70%-30% split between training and test sets.</p>
<p>The result is list of two desired data frames. For convenience, let’s give them short names:</p>
<pre class="r"><code>verTrain &lt;- myTrainTest$train  #the training set
verTest &lt;-  myTrainTest$test  #the test set</code></pre>
<p>Now you are ready to build your model, using the training set.</p>
<pre class="r"><code>verMod &lt;- tree(pitch_type ~ ., data = verTrain)
plot(verMod); text(verMod)</code></pre>
<p><img src="divideTrainTest_files/figure-html/unnamed-chunk-7-1.png" width="528" /></p>
<p>Let’s see how well the tree worked on the data used to build it:</p>
<pre class="r"><code>summary(verMod)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = pitch_type ~ ., data = verTrain)
## Variables actually used in tree construction:
## [1] &quot;speed&quot; &quot;pfx_x&quot; &quot;pfx_z&quot;
## Number of terminal nodes:  6 
## Residual mean deviance:  0.2612 = 2796 / 10710 
## Misclassification error rate: 0.03248 = 348 / 10714</code></pre>
<p>The misclassification rate was 3.248%.</p>
<p>The <code>tryTree()</code> function can be used to see how well the tree performs on the test set:</p>
<pre class="r"><code>tryTree(mod = verMod, testSet = verTest, 
        truth = verTest$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.2684 = 1231 / 4587
## Misclassification error rate:  0.03396 = 156 / 4593
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  760    0    2   19   11
##         CU    0  780    0    0   28
##         FF    3    0 1996   16    3
##         FT    0    0   46  570    0
##         SL    5   23    0    0  331</code></pre>
<p>As one might expect, the misclassification rate is a bit higher on the new data: about 3.4%.</p>
<p><strong>Note:</strong> It is possible to “try” the tree on the training set:</p>
<pre class="r"><code>tryTree(mod = verMod, testSet = verTrain, 
        truth = verTrain$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.2612 = 2796.4 / 10708
## Misclassification error rate:  0.03248 = 348 / 10714
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH 1767    0    1   37   40
##         CU    0 1875    0    0   73
##         FF   13    0 4607   36    4
##         FT    0    0  104 1343    0
##         SL    2   38    0    0  774</code></pre>
</div>
<div id="multiple-models-using-a-quiz-set" class="section level2">
<h2>Multiple Models: Using a Quiz Set</h2>
<p>More often than not, you would like to build several tree models and compare them, choosing the one that works best. If do the comparisons on the test set, then you are using the test set as <em>part</em> of the training set, so it won’t be “new data” any more. Hence when you plan to build and compare multiple models, the training set should be further subdivided into a set used to build all of the models and another set, called the <em>quiz</em> set, that provides “new data” on which the candidate models will be compared.</p>
<p>This results in a three-fold division of the available data:</p>
<ul>
<li>training set (build all models here)</li>
<li>quiz set (try out all models here and pick your favorite)</li>
<li>test set (test your favorite model here)</li>
</ul>
<p>This division can be accomplished by adding a new argument, <code>prop.quiz</code>, to the <code>divideTrainTest()</code> function:</p>
<pre class="r"><code>my3Frames &lt;- divideTrainTest(seed = 4040, prop.train = 0.6, 
                             prop.quiz = 0.2, data = ver2)</code></pre>
<p>The resulting division is 60% training, 20% quiz and 20% test.</p>
<p>Now you can build as many models as you like:</p>
<pre class="r"><code>verTrain &lt;- my3Frames$train
verQuiz &lt;- my3Frames$quiz
verTest &lt;- my3Frames$test
verMod1 &lt;- tree(pitch_type ~ ., data = verTrain,
                control = tree.control(
                  nobs = nrow(verTrain),
                  mincut = 100,
                  minsize = 200,
                  mindev = 0.1
                ))
summary(verMod1)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = pitch_type ~ ., data = verTrain, control = tree.control(nobs = nrow(verTrain), 
##     mincut = 100, minsize = 200, mindev = 0.1))
## Variables actually used in tree construction:
## [1] &quot;speed&quot; &quot;pfx_x&quot;
## Number of terminal nodes:  4 
## Residual mean deviance:  0.5243 = 4813 / 9180 
## Misclassification error rate: 0.1016 = 933 / 9184</code></pre>
<pre class="r"><code>tryTree(mod = verMod1, testSet = verQuiz, truth = verQuiz$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.5241 = 1602.1 / 3057
## Misclassification error rate:  0.10095 = 309 / 3061
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  493    0    0   14   13
##         CU    2  542    0    0  238
##         FF    3    0 1304    8    0
##         FT    0    0   31  413    0
##         SL    0    0    0    0    0</code></pre>
<p>The first tree we made has only four nodes, and misclassification rate on the quiz data is very high: about 10.1%.</p>
<p>Let’s make another, very large tree:</p>
<pre class="r"><code>verMod2 &lt;- tree(pitch_type ~ ., data = verTrain,
                control = tree.control(
                  nobs = nrow(verTrain),
                  mincut = 1,
                  minsize = 2,
                  mindev = 0
                ))
summary(verMod2)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = pitch_type ~ ., data = verTrain, control = tree.control(nobs = nrow(verTrain), 
##     mincut = 1, minsize = 2, mindev = 0))
## Number of terminal nodes:  165 
## Residual mean deviance:  0 = 0 / 9019 
## Misclassification error rate: 0 = 0 / 9184</code></pre>
<pre class="r"><code>tryTree(mod = verMod2, testSet = verQuiz, truth = verQuiz$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.0842 = 248.6 / 2952
## Misclassification error rate:  0.02581 = 79 / 3061
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  484    0    2    1   12
##         CU    0  532    0    0    4
##         FF    4    0 1318   21    0
##         FT    3    0   15  413    0
##         SL    7   10    0    0  235</code></pre>
<p>Model 2 has very many nodes, and although the misclassification rate on its own data is 0%, the rate on the quiz set is about 2.58%. That’s the rate we care about!</p>
<p>Let’s try a model that is “intermediate” in size:</p>
<pre class="r"><code>verMod3 &lt;- tree(pitch_type ~ ., data = verTrain,
                control = tree.control(
                  nobs = nrow(verTrain),
                  mincut = 5,
                  minsize = 10,
                  mindev = 0.0003
                ))
summary(verMod3)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = pitch_type ~ ., data = verTrain, control = tree.control(nobs = nrow(verTrain), 
##     mincut = 5, minsize = 10, mindev = 3e-04))
## Variables actually used in tree construction:
## [1] &quot;speed&quot;   &quot;pfx_x&quot;   &quot;strikes&quot; &quot;pz&quot;      &quot;pfx_z&quot;   &quot;px&quot;      &quot;pitches&quot;
## Number of terminal nodes:  48 
## Residual mean deviance:  0.07698 = 703.3 / 9136 
## Misclassification error rate: 0.0159 = 146 / 9184</code></pre>
<pre class="r"><code>tryTree(mod = verMod3, testSet = verQuiz, truth = verQuiz$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.1188 = 358.2 / 3016
## Misclassification error rate:  0.02221 = 68 / 3061
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  490    0    0    2   11
##         CU    0  533    0    0    3
##         FF    3    0 1315   15    0
##         FT    2    0   18  418    0
##         SL    3    9    2    0  237</code></pre>
<p>This tree has a lower misclassification rate: about 2.22%.</p>
<p>And finally, a tree that is rather on the small side:</p>
<pre class="r"><code>verMod4 &lt;- tree(pitch_type ~ ., data = verTrain,
                control = tree.control(
                  nobs = nrow(verTrain),
                  mincut = 5,
                  minsize = 10,
                  mindev = 0.01
                ))
summary(verMod4)</code></pre>
<pre><code>## 
## Classification tree:
## tree(formula = pitch_type ~ ., data = verTrain, control = tree.control(nobs = nrow(verTrain), 
##     mincut = 5, minsize = 10, mindev = 0.01))
## Variables actually used in tree construction:
## [1] &quot;speed&quot; &quot;pfx_x&quot; &quot;pfx_z&quot;
## Number of terminal nodes:  6 
## Residual mean deviance:  0.2632 = 2416 / 9178 
## Misclassification error rate: 0.0331 = 304 / 9184</code></pre>
<pre class="r"><code>tryTree(mod = verMod4, testSet = verQuiz, truth = verQuiz$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.2602 = 795.1 / 3055
## Misclassification error rate:  0.033 = 101 / 3061
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  493    0    0   14   13
##         CU    0  527    0    0   15
##         FF    3    0 1304    8    0
##         FT    0    0   31  413    0
##         SL    2   15    0    0  223</code></pre>
<p>This last tree has a reasonably low misclassification rate of about 3.3%, but that’s not the lowest rate we have seen. With only six nodes, though, Model 4 is small enough to plot and will be easier for non-technical people to understand.</p>
<p>Which tree should we go with? That’s a judgment call that depends on the practical context of the problem.</p>
<p>Let’s say that we decide to go with Model 4. Our last act is to test this model on the test set:</p>
<pre class="r"><code>tryTree(mod = verMod4, testSet = verTest, truth = verTest$pitch_type)</code></pre>
<pre><code>## Residual mean deviance:  0.2747 = 839.5 / 3056
## Misclassification error rate:  0.03396 = 104 / 3062
## Confusion matrix:
##           truth
## prediction   CH   CU   FF   FT   SL
##         CH  503    0    2   11   12
##         CU    0  526    0    0   18
##         FF    3    0 1348   11    3
##         FT    0    0   28  367    0
##         SL    1   15    0    0  214</code></pre>
<p>Based on the result, we figure that our chosen model will have about a 3.4% error rate on new but similar observations of Justin Verlander pitches.</p>
<p><strong>Note: remember to stick with the model you chose! Don’t go back and try another candidate model on your test set!</strong></p>
</div>
<div id="technical-notes" class="section level2">
<h2>Technical Notes</h2>
<p><code>tryTree()</code> computes deviance and residual mean deviance using the same approach that <code>summary()</code> uses when applied to a tree model. Thus for a classification tree deviance is first computed at each terminal node as:</p>
<p><span class="math display">\[-2 \sum_{k = 1}^K n_k \ln p_k,\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(K\)</span> is the number of levels of the response variable;</li>
<li><span class="math inline">\(n_k\)</span> is the number of observation of the <span class="math inline">\(k^{\mathrm{th}}\)</span> level at the node;</li>
<li><span class="math inline">\(p_k\)</span> is the proportion of all obervations at the node that are of the <span class="math inline">\(k^{\mathrm{th}}\)</span> level;</li>
<li>the term <span class="math inline">\(n_k \ln p_k\)</span> is taken to be 0 when <span class="math inline">\(p_k\)</span> is 0.</li>
</ul>
<p>The deviance proper is the the sum of the deviances at all of the terminal nodes.</p>
<p>For regression trees the deviance is first computed at each node as:</p>
<p><span class="math display">\[\sum_{i=1}^N (y_i - \bar{y})^2,\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of observations at the node;</li>
<li><span class="math inline">\(y_i\)</span> is the value of the response variable for the <span class="math inline">\(i^{\mathrm{th}}\)</span> observation at the node;</li>
<li><span class="math inline">\(\bar{y}\)</span> is the mean of the response variable for all observations at the node.</li>
</ul>
<p>The deviance proper is again the sum of the deviances at all of the terminal nodes.</p>
<p>For either type of tree, the mean residual deviance is:</p>
<p><span class="math display">\[\frac{\mathrm{deviance}}{n - m},\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations at terminal nodes and <span class="math inline">\(m\)</span> is the number of terminal nodes.</p>
<p>Note that deviance calculations only depend on observations at terminal nodes. Observations that fail to reach a terminal node due to missing values are not taken into account. Similarly, for classification trees the misclassification rate is computed using only the observations at terminal nodes.</p>
</div>



<!-- bizible -->
<script type="text/javascript" src="//cdn.bizible.com/scripts/bizible.js" async=""></script>


<!-- disqus -->
<!--  <div id="disqus_thread" style="margin-top: 45px;"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname
        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
